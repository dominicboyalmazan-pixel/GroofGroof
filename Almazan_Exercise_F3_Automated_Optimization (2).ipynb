{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installing important libraries"
      ],
      "metadata": {
        "id": "ht47UNBw9KMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets torch sklearn -q"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n  \n  \u001b[31m√ó\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n  \u001b[31m‚îÇ\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n  \u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n  \n  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n\n\u001b[31m√ó\u001b[0m Encountered error while generating package metadata.\n\u001b[31m‚ï∞‚îÄ>\u001b[0m See above for output.\n\n\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n\u001b[1;36mhint\u001b[0m: See above for details.\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbJ6qrmOveFx",
        "outputId": "b7d0fef3-5def-4da0-b84d-7f95ff2a64df"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I started by installing the core libraries needed to work with `transformer-based` models, especially since I‚Äôll be using MentalBERT later on for sentiment analysis. The transformers library gives access to a wide range of pre-trained NLP models and tools for tokenization, fine-tuning, and text generation. It‚Äôs basically the foundation that allows me to load and use advanced models without having to train them from scratch.\n",
        "\n",
        "The `datasets` library helps in handling large text datasets more efficiently. It makes it easy to load, split, and preprocess data, which is really useful when preparing text for BERT-based models. Meanwhile, I installed `torch` because it‚Äôs the deep learning framework that powers these transformer models  it handles all the computations that happen behind the scenes during model training and prediction.\n",
        "\n",
        "Lastly, I added `sklearn` since it includes tools for model evaluation and preprocessing that I‚Äôll still use alongside the transformer model. Adding the `-q` flag just runs the installation quietly, so it doesn‚Äôt flood the notebook with too much text output. Overall, this setup ensures that everything I need for deep learning and NLP is ready before I start working with MentalBERT.\n"
      ],
      "metadata": {
        "id": "SsKTm9tl78QU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login()\n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19f17f40eac842faa992787373efa51d"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "19f17f40eac842faa992787373efa51d",
            "b4ab14eeec9c4183a683aa1a226ee8d1",
            "9b21a84ecfaf4138b644319dc21c815d",
            "9634cb37856a407badb14c8b31713bfe",
            "a1b2cbe8b318475092b6d00988dcf377",
            "7cb119d94756496d90e49cc4b77dd8d9",
            "7fc67afc315c41698f1de75462f83033",
            "374831d385bf488fb296843cc7e9d388",
            "bff0398c66834722b760f00d2942816f",
            "0a8ee3d23c744962a67cda6f0a63a8b7",
            "8cfd452b9e704a8fb557bfaf04cf4d2b",
            "ae3b88629af041c5b84b0d77351b107f",
            "0a9cb0823c80462daa534d7352014a86",
            "3589dc5fa9174974baae0018570409bd",
            "3ea00024dc0e47a5b4a8111fc377d4c0",
            "d787e4e0f89e4823bed5493840c2337f",
            "1db23a05ce5a4f1c8595ab564214c880",
            "b0decee1132c4c6d8846e6327d3a504f",
            "afa7bdee976d4a429b55546873e7c1fb",
            "bc55f7d45f12480292ffd542aa7d1d9b"
          ]
        },
        "id": "PjgjUz5iyRHC",
        "outputId": "e62ae92b-38c0-4b7a-fba6-0436f4e9bb98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before using **MentalBERT**, I needed to log in to the Hugging Face Hub since this model requires an API token for access. Hugging Face hosts a lot of pre-trained models, including MentalBERT, and it uses authentication to manage who can download and use them. To get this token, I created a free Hugging Face account, went to my profile settings, and generated an access token from the ‚ÄúAccess Tokens‚Äù section.\n",
        "\n",
        "Once I had the token, I imported the login function from `huggingface_hub` and ran `login()`. This command opens a prompt where I entered my token, which then authenticates my session with the Hugging Face Hub. After logging in, I could securely load the MentalBERT model and its tokenizer without any permission issues."
      ],
      "metadata": {
        "id": "Mdg-oCU98AJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token Code : 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"
      ],
      "metadata": {
        "id": "flGc2syF0iB4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "njQMvGJD0aFy",
        "outputId": "a7de1f48-669b-4e11-ca98-db66c5658b4f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used this command to make sure the Transformers library is updated to its latest version. By adding the `-U` flag, it upgrades any older installation instead of just reinstalling the same one. This is important because newer versions of Transformers often include performance improvements, bug fixes, and updated model compatibility  especially when working with models like **MentalBERT** that rely on recent architecture updates.\n",
        "\n",
        "Keeping Transformers up to date ensures that I can use all the latest functions and tokenizer features without running into version conflicts. It also makes sure the library works smoothly with other dependencies like PyTorch and datasets."
      ],
      "metadata": {
        "id": "IhrEKkMW8Nzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch transformers scikit-learn pandas\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\nRequirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\nRequirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\nRequirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhqUTEft3RL9",
        "outputId": "fc4f2a27-c729-4462-b806-6aa5526eb05c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "4.57.1\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5FHOIktl44Ce",
        "outputId": "63f21ce4-6695-4041-f7a2-50556a0c9616"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WAN B API CODE : 95d47de64d7c30ab73ce317e099af2fb8cb0a24f"
      ],
      "metadata": {
        "id": "7V9IHTcI_fka"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explanation for Random Search and Grid Search"
      ],
      "metadata": {
        "id": "Pn9Ux1Yx7BJ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support\n",
        ")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "N-WYf9_d7Pry"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I started by importing all the main libraries needed for preparing, training, and evaluating the MentalBERT model. The `pandas` library is used for handling and exploring the dataset, especially for reading CSV files and managing text data efficiently. Then, I imported `torch` and `Dataset` from PyTorch since Transformers are built on top of it. The `Dataset` class helps convert my text data into a format that can be fed directly into the model during training.\n",
        "\n",
        "Next, I brought in tools from scikit-learn to help with data splitting and evaluation. `train_test_split` divides the dataset into training and testing portions, while `accuracy_score` and `precision_recall_fscore_support` will later help measure how well the model performs on the test data. These metrics give a more complete view of performance, especially for a binary classification task like detecting suicidal vs. normal statements.\n",
        "\n",
        "Finally, I imported everything from Transformers that‚Äôs specific to using MentalBERT. `AutoTokenizer` automatically handles tokenization based on the model we choose, converting text into the numerical format BERT understands. `AutoModelForSequenceClassification` loads the actual pre-trained model designed for classification tasks. The `Trainer` and `TrainingArguments` handle the training process  they make it easier to define how the model trains, tracks progress, and saves checkpoints. Together, these imports set up everything I need to fine-tune MentalBERT efficiently.\n"
      ],
      "metadata": {
        "id": "pbiadOLw7RDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1Ô∏è‚É£ Load dataset\n",
        "df = pd.read_csv(\"Cleaned_Combined_Data.csv\")\n",
        "\n",
        "TEXT_COL = \"statement\"\n",
        "LABEL_COL = \"status\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "RbTQpHdRQUgQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I loaded my cleaned dataset using pandas with the `read_csv() `function. This command reads the file named ***‚ÄúCleaned_Combined_Data.csv‚Äù*** and stores it in a DataFrame called `df`, making it easy to view, filter, and process the data later. This dataset contains the text statements and their corresponding mental health labels that I‚Äôll use to train and test the MentalBERT model.\n",
        "\n",
        "I created two variables, `TEXT_COL and LABEL_COL`, to clearly define which parts of the dataset I‚Äôll be working with. The `TEXT_COL` is set to \"statement\", which holds all the written texts or posts that will be analyzed by the model. The `LABEL_COL` is set to \"status\", which contains the actual categories or mental health conditions  in this case, ‚ÄúNormal‚Äù and ‚ÄúSuicidal.‚Äù\n",
        "**bold text**\n",
        "Doing this makes my code more organized and easier to maintain. Instead of repeating the column names throughout the code, I can just refer to these variables whenever I need to access the text or the label columns. It‚Äôs a simple step, but it helps make the workflow cleaner and less prone to errors, especially when adjusting or reusing the code later on.\n"
      ],
      "metadata": {
        "id": "IT4phNPtQX3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode string labels to integer IDs\n",
        "df[LABEL_COL] = df[LABEL_COL].astype('category')\n",
        "df['label_id'] = df[LABEL_COL].cat.codes\n",
        "label_mapping = dict(enumerate(df[LABEL_COL].cat.categories))\n",
        "print(\"‚úÖ Label mapping:\", label_mapping)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "EOJouz4RQib6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I converted the string labels in my dataset into numeric values so that the model can understand them. First, I changed the `status` column into a categorical type using `astype('category')`, which helps pandas recognize it as a set of fixed categories instead of plain text. Then, I created a new column called `label_id` using `cat.codes,` which automatically assigns an integer to each category  for example, ‚ÄúNormal‚Äù becomes 0 and ‚ÄúSuicidal‚Äù becomes 1.\n",
        "\n",
        "I also created a `label_mapping` dictionary to keep track of which number corresponds to which label. This is helpful later when I interpret the model‚Äôs predictions and need to translate the results back to readable text. Printing the label mapping confirms that everything was encoded correctly before moving forward with model training."
      ],
      "metadata": {
        "id": "TecB0Qi_Qfd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[TEXT_COL].tolist(),\n",
        "    df['label_id'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "iU23aBhrQoDM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I split my dataset into training and validation sets using `train_test_split()`. Here, I took the text data from the `statement` column and the numeric labels from the `label_id column`, then separated them into 80% for training and 20% for validation. The `test_size=0.2` means that one-fifth of the data will be used later to check how well the model performs on unseen examples.\n",
        "\n",
        "I also set `random_state=42` to make sure the split stays consistent every time I run the code. This helps with reproducibility, meaning I‚Äôll always get the same training and validation sets across runs. Doing this step ensures that the MentalBERT model will learn from one portion of the data while being tested fairly on another."
      ],
      "metadata": {
        "id": "Y-gzfyXnQoyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"mental/mental-bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "YZOmkVq0Q7H3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I set `model_name` to `\"mental/mental-bert-base-uncased\"` since that‚Äôs the exact name of the pre-trained **MentalBERT** model I‚Äôll be using from Hugging Face. This model is specifically designed for analyzing mental health‚Äìrelated text, which fits perfectly with my project‚Äôs goal of identifying suicidal and normal statements.\n",
        "\n",
        "After that, I loaded the tokenizer using `AutoTokenizer.from_pretrained(model_name)`. The tokenizer is what converts raw text into tokens  basically breaking the text into smaller pieces and turning them into numerical IDs that the model can understand. Using the same tokenizer that comes with the model ensures that the text is processed in the exact way MentalBERT was originally trained."
      ],
      "metadata": {
        "id": "pgEyehbCQ9as"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=64):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = int(self.labels[idx])\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Fkm9mzyVRAAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I created a custom dataset class called `SentimentDataset`, which inherits from **PyTorch‚Äôs** built-in Dataset class. This lets me organize my text and label data in a way that‚Äôs compatible with the MentalBERT model during training. Inside the `__init__ `method, I passed in the text data, the corresponding labels, the tokenizer, and a `max_len` parameter (which I set to 64). The `max_len` value limits the maximum number of tokens per input, making sure each text sample has a consistent length for the model to process efficiently.\n",
        "\n",
        "The `__len__` method simply returns how many samples are in the dataset. This is a required method for any PyTorch dataset because it helps the data loader know how many times to loop through the dataset during training. By returning `len(self.texts)`, I‚Äôm basically telling the model how many text samples it will be working with.\n",
        "\n",
        "Next, the `__getitem__` method handles how to retrieve each individual sample from the dataset. For each index, it grabs the text and its corresponding label, converts the text into a string, and ensures the label is in integer format. Then, it uses the tokenizer to convert the text into numerical form that the model can understand. Here, I set parameters like `truncation=True `to shorten long texts, `padding=\"max_length\"` to make all sequences the same length, and `return_tensors='pt'` to output the data as PyTorch tensors.\n",
        "\n",
        "Finally, I returned a dictionary containing three key elements: `input_ids, attention_mask,` and `labels`. The input_ids represent the tokenized text, the `attention_mask` tells the model which parts of the input are real words versus padding, and `labels` are the actual target outputs (either Normal or Suicidal). This structure ensures that when the data is loaded in batches, the model receives everything it needs to train\n"
      ],
      "metadata": {
        "id": "2kvhKD6dRMjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = SentimentDataset(val_texts, val_labels, tokenizer)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Br5UGqbZRRGp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I created two dataset objects  `train_dataset and val_dataset  `using the custom SentimentDataset class I defined earlier. The `train_dataset` contains the training texts and labels that the model will learn from, while the `val_dataset `holds the validation data that will be used to test how well the model performs on unseen samples. Both datasets use the same tokenizer to make sure the text is processed in a consistent way.\n",
        "\n",
        "By passing the text, labels, and tokenizer into the `SentimentDataset` class, each dataset automatically handles tokenization, padding, and truncation for every text entry. This means the data is already preprocessed and ready for **MentalBERT** to use. The dataset structure also makes it easy to load batches of data during training without having to manually tokenize or format the text every time.\n",
        "\n",
        "Splitting the data this way helps prevent overfitting since the model can be trained on one portion of the dataset and validated on another. It‚Äôs a clean setup that keeps the workflow organized, ensuring that both training and evaluation use the same processing pipeline and consistent input format.\n"
      ],
      "metadata": {
        "id": "9IkD31uyRRx4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = len(label_mapping)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "4vmPtrirRUGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I first defined `num_labels` by getting the length of `label_mapping`, which tells me how many unique categories my model needs to predict. Since this project only has two classes ‚Äî Normal and Suicidal the value of `num_labels` will be 2. Setting this variable ensures that the output layer of the model has the correct number of neurons to match the classification task.\n",
        "\n",
        "Next, I loaded the MentalBERT model using `AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)`. This command downloads the pre-trained MentalBERT weights and automatically configures the model for sequence classification. By including `num_labels`, the final layer is adjusted to handle exactly two output classes.\n",
        "\n",
        "Doing this lets me take advantage of MentalBERT‚Äôs pre-trained knowledge on mental health‚Äìrelated text, while still customizing it for my specific task  detecting whether a statement is Normal or Suicidal. It‚Äôs an efficient way to use a powerful model that already understands language patterns without having to train one from scratch.\n"
      ],
      "metadata": {
        "id": "N8EK9WmdRcro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze lower layers for faster fine-tuning\n",
        "for name, param in model.named_parameters():\n",
        "    if not name.startswith(\"classifier\") and not name.startswith(\"bert.encoder.layer.11\"):\n",
        "        param.requires_grad = False"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "_3CM5BHjRdp4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I decided to **freeze the lower layers of the MentalBERT** model to make fine-tuning faster and more focused. In transformer models like BERT, the lower layers capture general language patterns, while the higher layers and the classifier layer specialize in task-specific features. Since I‚Äôm fine-tuning on a relatively small dataset, I don‚Äôt need to retrain the entire network from scratch.\n",
        "\n",
        "The `for name, param in model.named_parameters()`: loop goes through all the parameters of the model. By checking the parameter names, I can selectively decide which ones to update during training. Specifically, I keep the classifier layer and the last encoder layer `(bert.encoder.layer.11)` trainable, because these layers are the most important for learning the distinctions between Normal and Suicidal statements.\n",
        "\n",
        "All other parameters have `requires_grad = False`, meaning they won‚Äôt be updated during backpropagation. Freezing these layers reduces training time and computational load, while still allowing the model to adjust its final representations to my specific classification task. This is a practical way to fine-tune a large pre-trained model efficiently.\n"
      ],
      "metadata": {
        "id": "h84kNnRhRmc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5Ô∏è‚É£ Define metrics\n",
        "def compute_metrics(p):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    acc = accuracy_score(p.label_ids, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "cUcP1MUXRnsV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The` compute_metrics` function is a custom evaluation metric function designed to assess the performance of the MentalBERT sentiment classifier during training and validation. It takes as input the predictions made by the model `(p.predictions)` and the corresponding true labels `(p.label_ids)`. Inside the function, the model‚Äôs predicted class for each input is obtained by selecting the index of the highest predicted probability using `argmax(-1)`. This gives a list of predicted labels that can be directly compared to the true labels. The function then calculates accuracy, which measures the proportion of correctly classified samples out of the total, providing an overall view of the model‚Äôs correctness.\n",
        "\n",
        "Next, the function uses the`precision_recall_fscore_support` method from Scikit-learn to compute precision, recall, and F1-score with a ‚Äúweighted‚Äù average. This averaging method accounts for label imbalance by weighting each class according to its frequency in the dataset, ensuring fair evaluation even if some classes (e.g., ‚ÄúNormal‚Äù vs. ‚ÄúSuicidal‚Äù) appear more often than others. Precision measures how many predicted positives were actually correct, recall measures how many actual positives were correctly identified, and the F1-score balances both metrics as a harmonic mean. Finally, the function returns a dictionary containing all four metrics‚Äîaccuracy, precision, recall, and F1‚Äîwhich allows the Hugging Face Trainer to automatically compute and log these scores during training and evaluation.\n"
      ],
      "metadata": {
        "id": "qtGFBXlyR376"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter set on Random Search"
      ],
      "metadata": {
        "id": "eWEneeypShVZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_space = {\n",
        "    \"learning_rate\": [1e-5, 2e-5, 3e-5, 5e-5],\n",
        "    \"per_device_train_batch_size\": [8, 16, 32],\n",
        "    \"num_train_epochs\": [2, 3, 4, 5],\n",
        "    \"weight_decay\": [0.0, 0.01, 0.05]\n",
        "}\n",
        "\n",
        "n_trials = 5  # Number of random experiments\n",
        "results = []"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "izuaPRHMSGhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part of the code defines the hyperparameter search space and sets up the configuration for conducting random search experiments.\n",
        "\n",
        "The dictionary `param_space` specifies different values for four important hyperparameters that affect model training. The `learning_rate` controls how much the model‚Äôs weights are updated during training‚Äîtoo high may cause instability, while too low may slow convergence. The `per_device_train_batch_size` defines how many samples are processed before updating the model weights, impacting memory usage and training speed. The `num_train_epochs` indicates how many full passes the model makes over the training dataset, and weight_decay helps regularize the model to prevent overfitting by penalizing large weights.\n",
        "\n",
        "The `variable n_trials = 5` means that five random combinations from the parameter space will be tested during the random search. Each trial will train and evaluate the model with a different random selection of hyperparameters. The list `results = []` initializes an empty container where the performance metrics (such as accuracy, precision, recall, and F1-score) of each trial will be stored for later comparison. This setup ensures the best-performing configuration can be identified after all experiments have been completed.\n"
      ],
      "metadata": {
        "id": "hix40o3bSQ5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter set for Grid Search"
      ],
      "metadata": {
        "id": "pA4nT33RS9sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_space = {\n",
        "    \"learning_rate\": [3e-5, 5e-5],\n",
        "    \"per_device_train_batch_size\": [8, 16],\n",
        "    \"num_train_epochs\": [3, 4],\n",
        "    \"weight_decay\": [0.01]\n",
        "}\n",
        "\n",
        "# Generate all combinations\n",
        "param_combinations = list(itertools.product(\n",
        "    param_space[\"learning_rate\"],\n",
        "    param_space[\"per_device_train_batch_size\"],\n",
        "    param_space[\"num_train_epochs\"],\n",
        "    param_space[\"weight_decay\"]\n",
        "))\n",
        "\n",
        "results = []\n",
        "\n",
        "print(f\"\\nüîç Total combinations to test: {len(param_combinations)}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "J2Llb5qLSldP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `param_space` dictionary defines a smaller, focused range of hyperparameter values to reduce computational load compared to a larger search. Here, `learning_rate` has two options (3e-5 and 5e-5), `per_device_train_batch_size `has two options (8 and 16), `num_train_epochs` has two options (3 and 4), and weight_decay is fixed at 0.01. Using itertools.product, the code generates all possible combinations of these hyperparameters. Each combination represents a unique configuration to be tested during Grid Search.\n",
        "\n",
        "The list `param_combinations` stores these combinations, while `results = []` initializes an empty list to collect performance metrics for each trial. The print statement displays the total number of combinations to be tested‚Äîin this case, 8 combinations so the user knows how many training/evaluation runs will be executed for the Grid Search experiment."
      ],
      "metadata": {
        "id": "LbDASGLvSvMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(n_trials):\n",
        "    print(f\"\\nüöÄ Running Random Search Trial {i+1}/{n_trials}\")\n",
        "\n",
        "    # Randomly select parameters\n",
        "    params = {k: random.choice(v) for k, v in param_space.items()}\n",
        "    print(\"üéØ Selected params:\", params)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results_trial_{i+1}\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_total_limit=0, # Change save_strategy=\"no\" to save_total_limit=0\n",
        "        learning_rate=params[\"learning_rate\"],\n",
        "        per_device_train_batch_size=params[\"per_device_train_batch_size\"],\n",
        "        per_device_eval_batch_size=params[\"per_device_train_batch_size\"],\n",
        "        num_train_epochs=params[\"num_train_epochs\"],\n",
        "        weight_decay=params[\"weight_decay\"],\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=False,\n",
        "        metric_for_best_model=\"f1\"\n",
        "    )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ynTodFDOTT48"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The for loop iterates `n_trials` times, where each iteration corresponds to a single random search trial. Inside the loop, the params dictionary is created by randomly selecting one value for each hyperparameter from param_space using `random.choice()`. This ensures that each trial tests a different, randomly chosen combination of learning rate, batch size, number of epochs, and weight decay. The selected hyperparameters are printed so you can track which configuration is being evaluated in each trial.\n",
        "\n",
        "Next, `TrainingArguments` from the Hugging Face Transformers library is instantiated with the randomly selected parameters. Key arguments include output_dir to store the trial‚Äôs results, eval_strategy=\"epoch\" to evaluate the model at the end of each epoch, save_total_limit=0 to avoid saving multiple checkpoints, and the hyperparameters from params such as `learning_rate, per_device_train_batch_size, num_train_epochs, and weight_decay`. Logging is enabled for monitoring progress, and `metric_for_best_model=\"f1\"` indicates that the F1-score would be used to identify the best-performing model if `load_best_model_at_end` were set to True. This setup prepares each trial for training and evaluation under a unique randomly selected hyperparameter configuration.\n"
      ],
      "metadata": {
        "id": "DIBDF6ajTA-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    metrics = trainer.evaluate()\n",
        "\n",
        "    result_entry = {\n",
        "        \"trial\": i + 1,\n",
        "        **params,\n",
        "        \"accuracy\": metrics.get(\"eval_accuracy\", 0),\n",
        "        \"precision\": metrics.get(\"eval_precision\", 0),\n",
        "        \"recall\": metrics.get(\"eval_recall\", 0),\n",
        "        \"f1\": metrics.get(\"eval_f1\", 0)\n",
        "    }\n",
        "\n",
        "    results.append(result_entry)\n",
        "\n",
        "# Save results to Excel\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_excel(\"RandomSearch_Results.xlsx\", index=False)\n",
        "print(\"\\n‚úÖ Random Search complete! Results saved to RandomSearch_Results.xlsx\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "3zTB6LUZTsAM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, a Trainer object from Hugging Face is instantiated using the selected model, the `trial-specific training_args`, the `train_dataset and val_dataset,` the tokenizer, and the custom compute_metrics function. The trainer.train() method fine-tunes the model using the current hyperparameter configuration, and `trainer.evaluate()` computes evaluation metrics on the validation set.\n",
        "\n",
        "Next, a dictionary `result_entry` is created to store the trial number, the hyperparameters used in this trial `(via **params)`, and the evaluation `metrics‚Äîaccuracy, precision, recall, and F1-score‚Äîretrieve`d from the metrics dictionary. This entry is appended to the results list. After all trials are completed, the results are converted into a Pandas DataFrame and saved to an Excel file named RandomSearch_Results.xlsx, making it easy to analyze and compare the performance of all trials. The final print statement confirms that the random search process has finished and the results are successfully saved."
      ],
      "metadata": {
        "id": "7rD95aXiT0YO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_trial = max(results, key=lambda x: x[\"f1\"])\n",
        "print(\"\\nüèÜ Best Trial Configuration:\")\n",
        "for k, v in best_trial.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "print(\"\\n‚úÖ Sucessful\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "jSgc4QqQTrcW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `max()` function iterates over the results list of dictionaries and uses a lambda function lambda x: x[\"f1\"] as the key to determine which trial achieved the highest F1-score. This ensures that the selected trial balances both precision and recall, which is particularly important for imbalanced datasets like sentiment classification of ‚ÄúNormal‚Äù vs. ‚ÄúSuicidal‚Äù labels.\n",
        "\n",
        "After finding the best trial, a for loop prints all details of that trial, including the trial number, the hyperparameters used, and the evaluation metrics (accuracy, precision, recall, and F1-score). The final print statement reminds the user that the results are now ready for further analysis in the accompanying Excel file, which can be used to prepare a detailed IEEE report comparing all random search experiments."
      ],
      "metadata": {
        "id": "Tr2vzuvpUIPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"\\nüí¨ Type a sentence to analyze (type 'quit' to exit)\\n\")\n",
        "\n",
        "while True:\n",
        "    text = input(\"Enter a sentence: \")\n",
        "    if text.lower() == \"quit\":\n",
        "        print(\"üëã Exiting.\")\n",
        "        break\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=64\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoding)\n",
        "        preds = torch.softmax(outputs.logits, dim=-1)\n",
        "        pred_label = torch.argmax(preds, dim=1).item()\n",
        "        confidence = preds[0][pred_label].item()\n",
        "\n",
        "    label_name = label_mapping[pred_label]\n",
        "    print(f\"üß† Prediction: {label_name} ({confidence:.2%} confidence)\\n\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "iogz0_b1UhCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, `model.eval()` sets the model to evaluation mode, disabling training-specific behaviors like dropout. The device is determined to use GPU if available, otherwise CPU, and the model is moved to that device with `model.to(device`) for efficient computation. The program then prints instructions, indicating that typing \"quit\" will exit the loop.\n",
        "\n",
        "Inside the` while True` loop, the code takes user input (text) and processes it using the tokenizer, which converts the text into token IDs suitable for the model. Padding and truncation ensure the input matches the model‚Äôs expected maximum sequence length of 64 tokens.` torch.no_grad()` disables gradient calculation to save memory and speed up inference. The model outputs logits, which are converted to probabilities using `torch.softmax()`. The predicted label is obtained with argmax, and its corresponding confidence score is extracted. Finally, the predicted label name is retrieved from label_mapping, and the prediction with confidence is printed to the user. This loop continues until the user types `\"quit\"`.\n"
      ],
      "metadata": {
        "id": "U-4c6JZ8UVOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Search full code ( With Result)"
      ],
      "metadata": {
        "id": "iX_MOIrwx6H0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# LIGHTWEIGHT MENTALBERT SENTIMENT CLASSIFIER WITH RANDOM SEARCH\n",
        "# ===============================\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support\n",
        ")\n",
        "\n",
        "# 1Ô∏è‚É£ Load dataset\n",
        "df = pd.read_csv(\"Cleaned_Combined_Data.csv\")\n",
        "\n",
        "TEXT_COL = \"statement\"\n",
        "LABEL_COL = \"status\"\n",
        "\n",
        "# Encode string labels to integers\n",
        "df[LABEL_COL] = df[LABEL_COL].astype('category')\n",
        "df['label_id'] = df[LABEL_COL].cat.codes\n",
        "label_mapping = dict(enumerate(df[LABEL_COL].cat.categories))\n",
        "print(\"‚úÖ Label mapping:\", label_mapping)\n",
        "\n",
        "# Split dataset\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[TEXT_COL].tolist(),\n",
        "    df['label_id'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 2Ô∏è‚É£ Load tokenizer\n",
        "model_name = \"mental/mental-bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 3Ô∏è‚É£ Dataset class\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=64):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = int(self.labels[idx])\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Prepare datasets\n",
        "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = SentimentDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "# 4Ô∏è‚É£ Load model\n",
        "num_labels = len(label_mapping)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "# ‚úÖ Freeze lower layers for faster training\n",
        "for name, param in model.named_parameters():\n",
        "    if not name.startswith(\"classifier\") and not name.startswith(\"bert.encoder.layer.11\"):\n",
        "        param.requires_grad = False\n",
        "\n",
        "# 5Ô∏è‚É£ Define metrics\n",
        "def compute_metrics(p):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    acc = accuracy_score(p.label_ids, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# ===============================\n",
        "# 6Ô∏è‚É£ RANDOM SEARCH HYPERPARAMETER TUNING\n",
        "# ===============================\n",
        "\n",
        "param_space = {\n",
        "    \"learning_rate\": [1e-5, 2e-5, 3e-5, 5e-5],\n",
        "    \"per_device_train_batch_size\": [8, 16, 32],\n",
        "    \"num_train_epochs\": [2, 3, 4, 5],\n",
        "    \"weight_decay\": [0.0, 0.01, 0.05]\n",
        "}\n",
        "\n",
        "n_trials = 5  # Number of random experiments\n",
        "results = []\n",
        "\n",
        "for i in range(n_trials):\n",
        "    print(f\"\\nüöÄ Running Random Search Trial {i+1}/{n_trials}\")\n",
        "\n",
        "    # Randomly select parameters\n",
        "    params = {k: random.choice(v) for k, v in param_space.items()}\n",
        "    print(\"üéØ Selected params:\", params)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results_trial_{i+1}\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_total_limit=0, # Change save_strategy=\"no\" to save_total_limit=0\n",
        "        learning_rate=params[\"learning_rate\"],\n",
        "        per_device_train_batch_size=params[\"per_device_train_batch_size\"],\n",
        "        per_device_eval_batch_size=params[\"per_device_train_batch_size\"],\n",
        "        num_train_epochs=params[\"num_train_epochs\"],\n",
        "        weight_decay=params[\"weight_decay\"],\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=False,\n",
        "        metric_for_best_model=\"f1\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    metrics = trainer.evaluate()\n",
        "\n",
        "    result_entry = {\n",
        "        \"trial\": i + 1,\n",
        "        **params,\n",
        "        \"accuracy\": metrics.get(\"eval_accuracy\", 0),\n",
        "        \"precision\": metrics.get(\"eval_precision\", 0),\n",
        "        \"recall\": metrics.get(\"eval_recall\", 0),\n",
        "        \"f1\": metrics.get(\"eval_f1\", 0)\n",
        "    }\n",
        "\n",
        "    results.append(result_entry)\n",
        "\n",
        "# Save results to Excel\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_excel(\"RandomSearch_Results.xlsx\", index=False)\n",
        "print(\"\\n‚úÖ Random Search complete! Results saved to RandomSearch_Results.xlsx\")\n",
        "\n",
        "# ===============================\n",
        "# 7Ô∏è‚É£ BEST MODEL FINAL EVALUATION (Optional)\n",
        "# ===============================\n",
        "\n",
        "best_trial = max(results, key=lambda x: x[\"f1\"])\n",
        "print(\"\\nüèÜ Best Trial Configuration:\")\n",
        "for k, v in best_trial.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "print(\"\\n‚úÖ You can now proceed to analyze the Excel file for your IEEE report.\")\n",
        "\n",
        "# ===============================\n",
        "# 8Ô∏è‚É£ USER INPUT PREDICTION (Optional interactive testing)\n",
        "# ===============================\n",
        "\n",
        "model.eval()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "print(\"\\nüí¨ Type a sentence to analyze (type 'quit' to exit)\\n\")\n",
        "\n",
        "while True:\n",
        "    text = input(\"Enter a sentence: \")\n",
        "    if text.lower() == \"quit\":\n",
        "        print(\"üëã Exiting.\")\n",
        "        break\n",
        "\n",
        "    encoding = tokenizer(\n",
        "        text,\n",
        "        return_tensors='pt',\n",
        "        truncation=True,\n",
        "        padding='max_length',\n",
        "        max_length=64\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encoding)\n",
        "        preds = torch.softmax(outputs.logits, dim=-1)\n",
        "        pred_label = torch.argmax(preds, dim=1).item()\n",
        "        confidence = preds[0][pred_label].item()\n",
        "\n",
        "    label_name = label_mapping[pred_label]\n",
        "    print(f\"üß† Prediction: {label_name} ({confidence:.2%} confidence)\\n\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "‚úÖ Label mapping: {0: 'Normal', 1: 'Suicidal'}\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nüöÄ Running Random Search Trial 1/5\nüéØ Selected params: {'learning_rate': 2e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 2, 'weight_decay': 0.05}\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipython-input-418805040.py:130: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n  | |_| | '_ \\/ _` / _` |  _/ -_)\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdominicboy-almazan\u001b[0m (\u001b[33msteven-tiu-jose-rizal-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "Tracking run with wandb version 0.22.3",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "Run data is saved locally in <code>/content/wandb/run-20251108_031651-b39vdyjp</code>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "Syncing run <strong><a href='https://wandb.ai/steven-tiu-jose-rizal-university/huggingface/runs/b39vdyjp' target=\"_blank\">worldly-meadow-88</a></strong> to <a href='https://wandb.ai/steven-tiu-jose-rizal-university/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": " View project at <a href='https://wandb.ai/steven-tiu-jose-rizal-university/huggingface' target=\"_blank\">https://wandb.ai/steven-tiu-jose-rizal-university/huggingface</a>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": " View run at <a href='https://wandb.ai/steven-tiu-jose-rizal-university/huggingface/runs/b39vdyjp' target=\"_blank\">https://wandb.ai/steven-tiu-jose-rizal-university/huggingface/runs/b39vdyjp</a>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='5400' max='5400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5400/5400 05:43, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.096100</td>\n      <td>0.101192</td>\n      <td>0.968513</td>\n      <td>0.968498</td>\n      <td>0.968513</td>\n      <td>0.968503</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.057600</td>\n      <td>0.099472</td>\n      <td>0.971106</td>\n      <td>0.971110</td>\n      <td>0.971106</td>\n      <td>0.971108</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [675/675 00:22]\n    </div>\n    ",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nüöÄ Running Random Search Trial 2/5\nüéØ Selected params: {'learning_rate': 1e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 4, 'weight_decay': 0.0}\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipython-input-418805040.py:130: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='10800' max='10800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10800/10800 10:35, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.034300</td>\n      <td>0.098250</td>\n      <td>0.974254</td>\n      <td>0.974271</td>\n      <td>0.974254</td>\n      <td>0.974261</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.053700</td>\n      <td>0.090445</td>\n      <td>0.974625</td>\n      <td>0.974613</td>\n      <td>0.974625</td>\n      <td>0.974609</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.105700</td>\n      <td>0.091591</td>\n      <td>0.975366</td>\n      <td>0.975358</td>\n      <td>0.975366</td>\n      <td>0.975361</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.019800</td>\n      <td>0.090951</td>\n      <td>0.975921</td>\n      <td>0.975910</td>\n      <td>0.975921</td>\n      <td>0.975911</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [675/675 00:22]\n    </div>\n    ",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nüöÄ Running Random Search Trial 3/5\nüéØ Selected params: {'learning_rate': 1e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 4, 'weight_decay': 0.0}\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipython-input-418805040.py:130: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='10800' max='10800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10800/10800 16:47, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.002900</td>\n      <td>0.095711</td>\n      <td>0.977774</td>\n      <td>0.977777</td>\n      <td>0.977774</td>\n      <td>0.977775</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.018700</td>\n      <td>0.088298</td>\n      <td>0.977588</td>\n      <td>0.977584</td>\n      <td>0.977588</td>\n      <td>0.977571</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.108600</td>\n      <td>0.091554</td>\n      <td>0.978515</td>\n      <td>0.978505</td>\n      <td>0.978515</td>\n      <td>0.978507</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.007400</td>\n      <td>0.091129</td>\n      <td>0.979070</td>\n      <td>0.979065</td>\n      <td>0.979070</td>\n      <td>0.979055</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [675/675 00:22]\n    </div>\n    ",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nüöÄ Running Random Search Trial 4/5\nüéØ Selected params: {'learning_rate': 1e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 4, 'weight_decay': 0.0}\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipython-input-418805040.py:130: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='10800' max='10800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10800/10800 17:18, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.000600</td>\n      <td>0.097665</td>\n      <td>0.979070</td>\n      <td>0.979066</td>\n      <td>0.979070</td>\n      <td>0.979068</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.003400</td>\n      <td>0.093723</td>\n      <td>0.979441</td>\n      <td>0.979493</td>\n      <td>0.979441</td>\n      <td>0.979408</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.080200</td>\n      <td>0.095538</td>\n      <td>0.980737</td>\n      <td>0.980739</td>\n      <td>0.980737</td>\n      <td>0.980721</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.002200</td>\n      <td>0.094892</td>\n      <td>0.980182</td>\n      <td>0.980188</td>\n      <td>0.980182</td>\n      <td>0.980163</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [675/675 00:22]\n    </div>\n    ",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nüöÄ Running Random Search Trial 5/5\nüéØ Selected params: {'learning_rate': 1e-05, 'per_device_train_batch_size': 8, 'num_train_epochs': 4, 'weight_decay': 0.0}\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipython-input-418805040.py:130: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='10800' max='10800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10800/10800 18:36, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.000400</td>\n      <td>0.105607</td>\n      <td>0.979996</td>\n      <td>0.979991</td>\n      <td>0.979996</td>\n      <td>0.979993</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.002100</td>\n      <td>0.104141</td>\n      <td>0.980182</td>\n      <td>0.980264</td>\n      <td>0.980182</td>\n      <td>0.980145</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.032800</td>\n      <td>0.106195</td>\n      <td>0.979996</td>\n      <td>0.980047</td>\n      <td>0.979996</td>\n      <td>0.979965</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.000700</td>\n      <td>0.104824</td>\n      <td>0.980737</td>\n      <td>0.980782</td>\n      <td>0.980737</td>\n      <td>0.980709</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [675/675 00:22]\n    </div>\n    ",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n‚úÖ Random Search complete! Results saved to RandomSearch_Results.xlsx\n\nüèÜ Best Trial Configuration:\ntrial: 5\nlearning_rate: 1e-05\nper_device_train_batch_size: 8\nnum_train_epochs: 4\nweight_decay: 0.0\naccuracy: 0.9807371735506575\nprecision: 0.980782169937334\nrecall: 0.9807371735506575\nf1: 0.9807090073863047\n\n‚úÖ You can now proceed to analyze the Excel file for your IEEE report.\n\nüí¨ Type a sentence to analyze (type 'quit' to exit)\n\nüß† Prediction: Suicidal (98.90% confidence)\n\nEnter a sentence: quit\nüëã Exiting.\n"
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iIzmYmp8dKF0",
        "outputId": "0efad7be-3fab-4b1f-a3cf-1a126f620859"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid Search"
      ],
      "metadata": {
        "id": "ANgpCoEPx8mP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# LIGHTWEIGHT MENTALBERT SENTIMENT CLASSIFIER WITH GRID SEARCH\n",
        "# ===============================\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import itertools\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments\n",
        ")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support\n",
        ")\n",
        "\n",
        "# 1Ô∏è‚É£ Load dataset\n",
        "df = pd.read_csv(\"Cleaned_Combined_Data.csv\")\n",
        "\n",
        "TEXT_COL = \"statement\"\n",
        "LABEL_COL = \"status\"\n",
        "\n",
        "# Encode string labels to integers\n",
        "df[LABEL_COL] = df[LABEL_COL].astype('category')\n",
        "df['label_id'] = df[LABEL_COL].cat.codes\n",
        "label_mapping = dict(enumerate(df[LABEL_COL].cat.categories))\n",
        "print(\"‚úÖ Label mapping:\", label_mapping)\n",
        "\n",
        "# Split dataset\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[TEXT_COL].tolist(),\n",
        "    df['label_id'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 2Ô∏è‚É£ Load tokenizer\n",
        "model_name = \"mental/mental-bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 3Ô∏è‚É£ Dataset class\n",
        "class SentimentDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=64):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = int(self.labels[idx])\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Prepare datasets\n",
        "train_dataset = SentimentDataset(train_texts, train_labels, tokenizer)\n",
        "val_dataset = SentimentDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "# 4Ô∏è‚É£ Define metrics\n",
        "def compute_metrics(p):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    acc = accuracy_score(p.label_ids, preds)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(p.label_ids, preds, average='weighted')\n",
        "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "\n",
        "# ===============================\n",
        "# 5Ô∏è‚É£ GRID SEARCH HYPERPARAMETER TUNING\n",
        "# ===============================\n",
        "\n",
        "param_space = {\n",
        "    \"learning_rate\": [3e-5, 5e-5],\n",
        "    \"per_device_train_batch_size\": [8, 16],\n",
        "    \"num_train_epochs\": [3, 4],\n",
        "    \"weight_decay\": [0.01]\n",
        "}\n",
        "\n",
        "# Generate all combinations\n",
        "param_combinations = list(itertools.product(\n",
        "    param_space[\"learning_rate\"],\n",
        "    param_space[\"per_device_train_batch_size\"],\n",
        "    param_space[\"num_train_epochs\"],\n",
        "    param_space[\"weight_decay\"]\n",
        "))\n",
        "\n",
        "results = []\n",
        "\n",
        "print(f\"\\nüîç Total combinations to test: {len(param_combinations)}\")\n",
        "\n",
        "for i, (lr, batch_size, epochs, wd) in enumerate(param_combinations, 1):\n",
        "    print(f\"\\nüöÄ Running Grid Search Trial {i}/{len(param_combinations)}\")\n",
        "    print(f\"üéØ Params: LR={lr}, Batch={batch_size}, Epochs={epochs}, Weight Decay={wd}\")\n",
        "\n",
        "    # Reload model each iteration\n",
        "    num_labels = len(label_mapping)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
        "\n",
        "    # Freeze lower layers for faster training\n",
        "    for name, param in model.named_parameters():\n",
        "        if not name.startswith(\"classifier\") and not name.startswith(\"bert.encoder.layer.11\"):\n",
        "            param.requires_grad = False\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./grid_results_trial_{i}\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"no\", # Changed from evaluation_strategy\n",
        "        learning_rate=lr,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=wd,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=10,\n",
        "        load_best_model_at_end=False,\n",
        "        metric_for_best_model=\"f1\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    metrics = trainer.evaluate()\n",
        "\n",
        "    result_entry = {\n",
        "        \"trial\": i,\n",
        "        \"learning_rate\": lr,\n",
        "        \"per_device_train_batch_size\": batch_size,\n",
        "        \"num_train_epochs\": epochs,\n",
        "        \"weight_decay\": wd,\n",
        "        \"accuracy\": metrics.get(\"eval_accuracy\", 0),\n",
        "        \"precision\": metrics.get(\"eval_precision\", 0),\n",
        "        \"recall\": metrics.get(\"eval_recall\", 0),\n",
        "        \"f1\": metrics.get(\"eval_f1\", 0)\n",
        "    }\n",
        "\n",
        "    results.append(result_entry)\n",
        "\n",
        "# Save results to Excel\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_excel(\"GridSearch_Results.xlsx\", index=False)\n",
        "print(\"\\n‚úÖ Grid Search complete! Results saved to GridSearch_Results.xlsx\")\n",
        "\n",
        "# ===============================\n",
        "# 6Ô∏è‚É£ BEST MODEL SELECTION\n",
        "# ===============================\n",
        "\n",
        "best_trial = max(results, key=lambda x: x[\"f1\"])\n",
        "print(\"\\nüèÜ Best Grid Search Configuration:\")\n",
        "for k, v in best_trial.items():\n",
        "    print(f\"{k}: {v}\")\n",
        "\n",
        "print(\"\\n‚úÖ Complete\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "‚úÖ Label mapping: {0: 'Normal', 1: 'Suicidal'}\n\nüîç Total combinations to test: 8\n\nüöÄ Running Grid Search Trial 1/8\nüéØ Params: LR=3e-05, Batch=8, Epochs=3, Weight Decay=0.01\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipython-input-4033805790.py:134: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='8100' max='8100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8100/8100 07:57, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.081500</td>\n      <td>0.099013</td>\n      <td>0.971661</td>\n      <td>0.971664</td>\n      <td>0.971661</td>\n      <td>0.971662</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.071100</td>\n      <td>0.084789</td>\n      <td>0.975736</td>\n      <td>0.975734</td>\n      <td>0.975736</td>\n      <td>0.975713</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.093800</td>\n      <td>0.086624</td>\n      <td>0.976848</td>\n      <td>0.976842</td>\n      <td>0.976848</td>\n      <td>0.976829</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [675/675 00:24]\n    </div>\n    ",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nüöÄ Running Grid Search Trial 2/8\nüéØ Params: LR=3e-05, Batch=8, Epochs=4, Weight Decay=0.01\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipython-input-4033805790.py:134: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='10800' max='10800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10800/10800 08:58, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.099200</td>\n      <td>0.097917</td>\n      <td>0.973514</td>\n      <td>0.973503</td>\n      <td>0.973514</td>\n      <td>0.973507</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.050800</td>\n      <td>0.082761</td>\n      <td>0.975921</td>\n      <td>0.975918</td>\n      <td>0.975921</td>\n      <td>0.975900</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.137400</td>\n      <td>0.087715</td>\n      <td>0.977033</td>\n      <td>0.977041</td>\n      <td>0.977033</td>\n      <td>0.977036</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.026100</td>\n      <td>0.086383</td>\n      <td>0.978515</td>\n      <td>0.978507</td>\n      <td>0.978515</td>\n      <td>0.978502</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [675/675 00:22]\n    </div>\n    ",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nüöÄ Running Grid Search Trial 3/8\nüéØ Params: LR=3e-05, Batch=16, Epochs=3, Weight Decay=0.01\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipython-input-4033805790.py:134: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='4050' max='4050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4050/4050 06:00, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.127700</td>\n      <td>0.087276</td>\n      <td>0.970180</td>\n      <td>0.970166</td>\n      <td>0.970180</td>\n      <td>0.970151</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.082900</td>\n      <td>0.077469</td>\n      <td>0.974810</td>\n      <td>0.974798</td>\n      <td>0.974810</td>\n      <td>0.974795</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.080100</td>\n      <td>0.078211</td>\n      <td>0.975551</td>\n      <td>0.975539</td>\n      <td>0.975551</td>\n      <td>0.975536</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='338' max='338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [338/338 00:20]\n    </div>\n    ",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nüöÄ Running Grid Search Trial 4/8\nüéØ Params: LR=3e-05, Batch=16, Epochs=4, Weight Decay=0.01\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipython-input-4033805790.py:134: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='5400' max='5400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5400/5400 08:02, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.115100</td>\n      <td>0.087445</td>\n      <td>0.971106</td>\n      <td>0.971090</td>\n      <td>0.971106</td>\n      <td>0.971084</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.078700</td>\n      <td>0.076366</td>\n      <td>0.974995</td>\n      <td>0.974983</td>\n      <td>0.974995</td>\n      <td>0.974983</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.058900</td>\n      <td>0.080569</td>\n      <td>0.974995</td>\n      <td>0.975084</td>\n      <td>0.974995</td>\n      <td>0.975018</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.111200</td>\n      <td>0.075545</td>\n      <td>0.977403</td>\n      <td>0.977397</td>\n      <td>0.977403</td>\n      <td>0.977386</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='338' max='338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [338/338 00:20]\n    </div>\n    ",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nüöÄ Running Grid Search Trial 5/8\nüéØ Params: LR=5e-05, Batch=8, Epochs=3, Weight Decay=0.01\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipython-input-4033805790.py:134: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='8100' max='8100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [8100/8100 06:40, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.079000</td>\n      <td>0.097286</td>\n      <td>0.973884</td>\n      <td>0.973890</td>\n      <td>0.973884</td>\n      <td>0.973887</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.028400</td>\n      <td>0.077349</td>\n      <td>0.977774</td>\n      <td>0.977773</td>\n      <td>0.977774</td>\n      <td>0.977753</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.090900</td>\n      <td>0.081685</td>\n      <td>0.979811</td>\n      <td>0.979805</td>\n      <td>0.979811</td>\n      <td>0.979798</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [675/675 00:22]\n    </div>\n    ",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nüöÄ Running Grid Search Trial 6/8\nüéØ Params: LR=5e-05, Batch=8, Epochs=4, Weight Decay=0.01\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipython-input-4033805790.py:134: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='10800' max='10800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [10800/10800 08:53, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.070900</td>\n      <td>0.096014</td>\n      <td>0.974440</td>\n      <td>0.974433</td>\n      <td>0.974440</td>\n      <td>0.974436</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.041400</td>\n      <td>0.075271</td>\n      <td>0.977959</td>\n      <td>0.978007</td>\n      <td>0.977959</td>\n      <td>0.977924</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.133000</td>\n      <td>0.080154</td>\n      <td>0.980737</td>\n      <td>0.980730</td>\n      <td>0.980737</td>\n      <td>0.980730</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.007200</td>\n      <td>0.080827</td>\n      <td>0.981293</td>\n      <td>0.981291</td>\n      <td>0.981293</td>\n      <td>0.981279</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='675' max='675' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [675/675 00:22]\n    </div>\n    ",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nüöÄ Running Grid Search Trial 7/8\nüéØ Params: LR=5e-05, Batch=16, Epochs=3, Weight Decay=0.01\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipython-input-4033805790.py:134: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='4050' max='4050' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4050/4050 06:02, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.126200</td>\n      <td>0.082254</td>\n      <td>0.972588</td>\n      <td>0.972587</td>\n      <td>0.972588</td>\n      <td>0.972556</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.079500</td>\n      <td>0.069258</td>\n      <td>0.977033</td>\n      <td>0.977023</td>\n      <td>0.977033</td>\n      <td>0.977019</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.061700</td>\n      <td>0.070030</td>\n      <td>0.979255</td>\n      <td>0.979252</td>\n      <td>0.979255</td>\n      <td>0.979240</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='338' max='338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [338/338 00:20]\n    </div>\n    ",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nüöÄ Running Grid Search Trial 8/8\nüéØ Params: LR=5e-05, Batch=16, Epochs=4, Weight Decay=0.01\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipython-input-4033805790.py:134: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='5400' max='5400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [5400/5400 08:00, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.102700</td>\n      <td>0.082515</td>\n      <td>0.972588</td>\n      <td>0.972572</td>\n      <td>0.972588</td>\n      <td>0.972573</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.070600</td>\n      <td>0.069187</td>\n      <td>0.977588</td>\n      <td>0.977579</td>\n      <td>0.977588</td>\n      <td>0.977576</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.041700</td>\n      <td>0.071709</td>\n      <td>0.978329</td>\n      <td>0.978384</td>\n      <td>0.978329</td>\n      <td>0.978344</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.080500</td>\n      <td>0.068425</td>\n      <td>0.980737</td>\n      <td>0.980746</td>\n      <td>0.980737</td>\n      <td>0.980718</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='243' max='338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [243/338 00:14 < 00:05, 16.26 it/s]\n    </div>\n    ",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='338' max='338' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [338/338 00:20]\n    </div>\n    "
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\n‚úÖ Grid Search complete! Results saved to GridSearch_Results.xlsx\n\nüèÜ Best Grid Search Configuration:\ntrial: 6\nlearning_rate: 5e-05\nper_device_train_batch_size: 8\nnum_train_epochs: 4\nweight_decay: 0.01\naccuracy: 0.981292832005927\nprecision: 0.9812912739247692\nrecall: 0.981292832005927\nf1: 0.9812794158918405\n\n‚úÖ Complete\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4nTwHlBBx-nu",
        "outputId": "91951659-7a93-4eac-93bc-7db7a6c0337e"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python",
      "language": "python",
      "display_name": "Pyolite (preview)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9b21a84ecfaf4138b644319dc21c815d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "PasswordModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "PasswordView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_0a8ee3d23c744962a67cda6f0a63a8b7",
            "value": "",
            "style": "IPY_MODEL_8cfd452b9e704a8fb557bfaf04cf4d2b",
            "placeholder": "‚Äã",
            "_view_count": null,
            "continuous_update": true,
            "_model_module_version": "1.5.0",
            "disabled": false,
            "description": "Token:"
          }
        },
        "9634cb37856a407badb14c8b31713bfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "CheckboxModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "CheckboxView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_ae3b88629af041c5b84b0d77351b107f",
            "indent": true,
            "value": true,
            "style": "IPY_MODEL_0a9cb0823c80462daa534d7352014a86",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "disabled": false,
            "description": "Add token as git credential?"
          }
        },
        "b4ab14eeec9c4183a683aa1a226ee8d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "HTMLModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "HTMLView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_374831d385bf488fb296843cc7e9d388",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>",
            "style": "IPY_MODEL_bff0398c66834722b760f00d2942816f",
            "placeholder": "‚Äã",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "b0decee1132c4c6d8846e6327d3a504f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "LabelModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "LabelView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_afa7bdee976d4a429b55546873e7c1fb",
            "value": "Connecting...",
            "style": "IPY_MODEL_bc55f7d45f12480292ffd542aa7d1d9b",
            "placeholder": "‚Äã",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "d787e4e0f89e4823bed5493840c2337f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "ae3b88629af041c5b84b0d77351b107f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "bc55f7d45f12480292ffd542aa7d1d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3589dc5fa9174974baae0018570409bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "0a8ee3d23c744962a67cda6f0a63a8b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "1db23a05ce5a4f1c8595ab564214c880": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0a9cb0823c80462daa534d7352014a86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7fc67afc315c41698f1de75462f83033": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": "flex",
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": "50%",
            "grid_area": null,
            "align_items": "center",
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": "column",
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "afa7bdee976d4a429b55546873e7c1fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "a1b2cbe8b318475092b6d00988dcf377": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_model_module": "@jupyter-widgets/controls",
            "tooltip": "",
            "button_style": "",
            "_view_name": "ButtonView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_3589dc5fa9174974baae0018570409bd",
            "style": "IPY_MODEL_3ea00024dc0e47a5b4a8111fc377d4c0",
            "_view_count": null,
            "icon": "",
            "_model_module_version": "1.5.0",
            "disabled": false,
            "description": "Login"
          }
        },
        "bff0398c66834722b760f00d2942816f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "374831d385bf488fb296843cc7e9d388": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "3ea00024dc0e47a5b4a8111fc377d4c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "ButtonStyleModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "StyleView",
            "_view_module": "@jupyter-widgets/base",
            "font_weight": "",
            "_view_count": null,
            "button_color": null,
            "_model_module_version": "1.5.0"
          }
        },
        "19f17f40eac842faa992787373efa51d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "VBoxView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_7fc67afc315c41698f1de75462f83033",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "box_style": "",
            "children": []
          }
        },
        "7cb119d94756496d90e49cc4b77dd8d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "HTMLModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "HTMLView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_d787e4e0f89e4823bed5493840c2337f",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>",
            "style": "IPY_MODEL_1db23a05ce5a4f1c8595ab564214c880",
            "placeholder": "‚Äã",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "8cfd452b9e704a8fb557bfaf04cf4d2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "kernel_info": {
      "name": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}